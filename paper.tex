\documentclass[twoside,11pt]{article}
\usepackage{isipta}
%% Comment these two lines if you don't need UTF8
\usepackage[utf8]{inputenc}
\inputencoding{utf8} 
% Note that pgm.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
% It also sets the bibliographystyle 
%% Put here the import commands of the packages you need and your custom commands 
\usepackage{hyperref,color,soul,booktabs,bm,algpseudocode,algorithm,tikz,pgfplots}
\usepackage{xargs}                      % Use more than one optional parameter in a new commands
%\usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.
% TIKZ for drawing
\usetikzlibrary{shapes.geometric} % for diamond nodes shape
\usepgflibrary{shapes.arrows} % for latex arrow shape
\newtheorem{mydef}[theorem]{Definition}
\newtheorem{myex}[theorem]{Example}
\setulcolor{blue}
\newcommand{\BibTeX}{\textsc{B\kern-0.1emi\kern-0.017emb}\kern-0.15em\TeX}
% Running title and authors 
\ShortHeadings{Online Variable Elimination for Credal Polytrees}{De Bock, Huber, and Antonucci}
%\pgmheading{1}{2000}{1-48}{4/00}{10/00}{Professorson and Teacherman}
%\firstpageno{10}
\usepackage{todonotes}
\presetkeys{todonotes}{fancyline, color=blue!30}{}
%\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\begin{document}
% Title and authors
\title{Online Variable Elimination for Credal Polytrees}
\author{\name Jasper De Bock \email jasper.debock@ugent.be\\
\addr Ghent University, IDLab\\
Belgium\\
\AND
\name David Huber \email david@idsia.ch\\
\name Alessandro Antonucci \email alessandro@idsia.ch\\
\addr Istituto Dalle Molle di Studi Sull'Intelligenza Artificiale (IDSIA)\\
Switzerland}
\maketitle
\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
We consier a variable-elimination procedure for exact inference in singly connected credal networks with strong independence. Compared to earlier works in this direction, the necessary extensive specification of the local models and the inner points removal are performed in an online fashion. This might produce considerable savings in memory and, for the inner points removal only, time. Moreover, we show that, with singly connected networks, the inner points removal from the intermediate results of the inference can be achieved before any elimination in a space with the same dimensionality of the variable to eliminate. This prevents computational issues related to convex hulling in high dimensional spaces. Despite the NP-hardness of the task, which concerns even singly connected topologies, the experiments on a benchmark of randomly generated credal polytrees shows that exact inference can be achieved in separately specified models up to xxx variables, xxx states, and indegree xxx. Faster approximate results can be achieved by simple rounding the values of the extreme points. Finally, multiply connected models can be also processed, with an outer approximation, with loopset conditioning.
\end{abstract}
\begin{keywords}
Credal networks; convex hull; variable elimination; cutset conditioning.
\end{keywords}
\input{intro}
\input{basics}
\input{VEdummy}
\input{VEsmart}
\input{example}
%\input{experiments}
\section{Extending the algorithm}
\subsection{Rounding the potentials}
The above considered procedure can be used to credal updating in polytree shaped CNs. Yet the problem, even in the case of marginal computation. Consider for instance the problem used by xxx to prove that marginal inference in ternary polytrees is NP-hard. Following the directions in xxx, we achieve an approximated.
\subsection{Multiply-connected networks}
Yet, it is not applicable to multiply connected models and, with even with simple polytrees it might
\section{Experiments}
In order to demonstrate our algorithm we consider a benchmark of xxx. Note that unlike the experiments of xxx, we consider separately specified credal networks which are extensivised as in xxx when necessary (see line xxx of alg xx).
\section{Conclusions and Outlooks}
Let me see. Mention that we might explore other (more complex) inference task such as the computation of the probability of evidence as well as the maximum a posteriori hypothesis task (MAP), which in the credal setting can be formulated in different ways.
\bibliography{biblio}
\acks{We would like to acknowledge support for this project from the Random Science Foundation.}
\end{document}
%When the procedure end the algorithm returns $P(X_0,x_E)$ which can be used to compute $P(X_0|x_E)$ by a simple marginalization. The combination operation simply consists in the product of two CPTs, marginalisation simply coincides with the sum of a variable on the left and focusing with the xxx.
%For sets of conditional mass functions we use the short notation $P(X_I|X_J)=\{ P(X_I|x_J) \}_{x_J}$. Such a collection of mass functions can be clearly regarded as a real-valued functions of either the variables on the left and the right of the conditioning bar. For CSs we analougously use notation $K(X_I|X_J)$ for a collection of credal sets. Yet, in this case the position wrt conditioning bar is not irrelevant. From an algebraic point of view this is a set-valued function of $X_I$ separately for each value of $X_J$. To bypass this issue it is possible to formulate each local specification of conditional . This is for instance the approach xxx. We use notation $K(X_I||X_J)$ to denote an extensive formulation of $K(X_I|X_J)$. Note that if on a side the extensive specification allows for xxx, it might be highly redundant xxx.
%Variable elimination is a standard inference strategy for xxx.
%Let us first explain how the computation of a marginal (i.e., $X_E = \emptyset$). Without lack of generality let us assume that the queried variable is $X_0$. 
%Chain and reversed chain
%\begin{myex}
%Let $X$ and $Y$ be two Boolean variables. If $P(X|Y=0)=\mathrm{CH}[]$ and $K(X|Y=1)=$ Then the xxx.
%\end{myex}
%\section{Cutset conditioning}
%The VE procedure described in the previous section (whose worst case complexity remains exponentials) can be applied to polytree only. In order to extends these ideas to multiply connected topologies we adopt the same strategy considered for credal networks and based on the notion of cutset. It is a well-known fact that the arcs leaving an observed node in a Bayesian networks can be removed, provided that the local models of its children are replaced to xxx. This results have been shown to be valid for credal networks with strong independence by xxx. The cutset conditioning idea simply consists in detecting a set of nows whose observations. For Bayesian networks this procedure, whose complexity is xxx.
%Just notice that $P(X_q,x_E)$ shoul be eventually normalised in order to obtain $P(X_q|x_E)$.
%From the point of view of variable elimination, there is a crucial differ
%\section{Variable elimination with credal networks}
%The combination operator $\otimes$ is achieved by simple elementwise multiplication. As noticed by Koller, this combination does not necessarily produce a set of conditional mass function.
%To extend the VE algorithm to the CNs framework we need to extend to the credal sets framework the above considered operations of combination and marginalization. This is done in the following definitions.
%\begin{mydef}
%	Let $K(X_I|X_J)$ and  denote a collection of credal is denotes as $K(X||Y)$. We call this transformation extensivisation.
%\end{mydef}
%Consider an updating task $\underline{P}(x_q|x_E)$ in a credal network. If the network is not singly connected we detect a set of cutset conditioning $X_F$. Thus we compute $K(X_q,x_E,x_F)$ and hence $K(X_q,x_E|x_F)$.
%\[ \underline{P}(x_q|x_E) \leq \sum_{x_F} \underline{P}(x_q,x_F|x_E) \]
%\section{Experiments}
%\subsection{Randomly generated credal sets}
%\subsection{The benchmark}
%\subsection{Results}
%Exact BNs inference is NP-hard task in general. Network topology is primarly setting this complexity level. In fact polytree shaped BNs can be updated in polynomial time (provided the the indegree is bounded), while general models can be only xx. CNs extend BNs by allowing for set-valued specification of the local models, with single conditional probability mass functions in the conditional probability tables replaced by convex sets of them. Updating in these cases is intended as the computation of the lower and upper bounds of the posterior probability with respect. This is clearly still a hard task extending BNs inference. Exact inference in credal networks is considerably more difficult. A recent paper. A possible rationale about that is the fact also the vertices are involved in that.
%%%%%%%%%%%
%\appendix
%\section{Proofs}
%\label{app:theorem}
%In this appendix we present a random filler.
%\vskip 0.2in
%\end{document}
